1.数据归一化
数据进行SOM或者其他机器学习算法之前一般先要归一化，一般分为两种，以学习[1 4 2]为例
                                                                 [2 0 0] 
一.
样本与样本之间因计算产生不必要的差别，即，我们想关注不同样本之间特征的相对差距，如对于样本一[1 4 2]而言，我们关注其中“1单位量、4单位量、2单位量”，
其等价于[2 8 4]，那我们需要对行向量进行归一化
二.
样本各特征之间因计算等问题产生了不必要的差别。即，我们希望各特征之间没有权重差别，但计算时却不可避免的产生了数值缩放，导致了权重的变化，
如对（4,0）T和（2,0）T而言，我们不关注这种4与2之间两倍的差别，则可利用归一化让其成为两个（1,0）T的向量，只关注不同样本之间特征的差别。
为了得到没有特征权重影响的各样本，我们需对每个列向量进行归一化。

首先可以肯定的是，输入向量全部需要归一化。具体要采取哪一种归一化方式，还是取决于应用环境。如选择采取第一种归一化方式，
那么神经网络也要保证在每次参与运算前进行归一化。即，对于网络训练过程来说，输入一个向量，归一化后与每个结点自带权值进行比较，
比较前结点权值明显需要归一化，比较后，在调整域内改变各结点权值，此时参与调整运算的输入向量和结点权值自身，都必须是已归一化处理的。
而调整后，明显权值又不是单位向量了，此时并不用着急进行归一化，因为下次比较前还有机会处理。对于分类过程中的各向量而言，
同样，比较前进行归一化即可。所以各个地方都不要遗漏。
若采取第二种归一化方式，那么我们可以在样本向量变化的范围内随机生成网络各结点权值。而此时，拥有不同权值的结点蕴含不同的特征信息，
所以不需要对网络结点权值进行归一化。

至于向量归一化的原理，可参考http://blog.sina.com.cn/s/blog_66a6172c0102v3em.html。
讲真，实验完成后发现，归一化和未归一化的结果差距还真是挺明显的


经典语句:
data = np.apply_along_axis(lambda x: x/np.linalg.norm(x), 1, data)

